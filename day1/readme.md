# Transformer 的心脏 —— Self-Attention 与 MHA

## 理论
### 注意力机制
- 核心公式： attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V  为什么除以 sqrt(d_k)
  - 为了控制 softmax 函数的输出范围，防止梯度消失或爆炸。
  - 假设 Q, K 是随机初始化的，它们的点积的期望为 0，方差为 d_k。
  - 除以 sqrt(d_k) 后，点积的期望为 0，方差为 1，符合 softmax 函数的输入要求。
  - 假设 Q 和 K 中的元素是独立的随机变量，均值为 0，方差为 1。那么，qi · ki 的均值为 0，方差为 1。Q · K = Σ(i=1 到 dk) (qi · ki)。根据方差的可加性，加和后的方差变成了 dk。标准差（Standard Deviation）变成了 √dk。Softmax 的梯度瓶颈：如果输入值的方差很大（比如 dk=64，标准差就是 8），有些点积结果会非常大（如 20），有些非常小（如 -20）。Softmax 函数 e^x / Σe^x 在 x 很大或很小时，梯度几乎为 0（你可以想象 Sigmoid 函数两端的平坦区域）。

除以 √dk 就是为了把方差拉回 1，让数值落在 Softmax 梯度敏感的区域（-2 到 2 之间），保证反向传播时梯度能流得动。
- mask 机制 padding mask和look ahead mask如何实现，在softmax之前加什么值
  - padding mask：QK之后将Attention Scores的padding位置设为一个非常小的数（如 -inf），在softmax之后，这些位置的注意力权重会被设为 0，从而忽略 padding 位置的影响。
  - look ahead mask：用于自注意力机制，防止模型在预测当前位置时，使用未来位置的信息。
- 复杂度 时间和空间复杂度多少，如何计算
  - 时间复杂度：O(n^2 * d)，其中 n 是序列的长度。
  - 空间复杂度：O(n^2)，其中 n 是序列的长度。
  - QKV矩阵式的维度为(n, d_k) 其中n是序列的长度，d_k是每个位置的向量维度，计算Q点乘K时，Q的维度为(n, d_k)，K的维度为(n, d_k)，时间复杂度为O(n^2 * d)，空间复杂度为O(n^2)。softmax的复杂度为O(n^2),点乘V的复杂度为（n^2 * d）
### 多头注意力MHA
- 原理：将QK矩阵分分割投影为不同的注意力头，用来关注信息的不同维度(n, n_h, d_h)
  - 为什么多头注意力更好？因为每个头用来关注信息的不同维度，从而提高模型的表示能力。
  - 输入向量x并不是直接切开，而是通过权重矩阵映射到不同的子空间再切分。
