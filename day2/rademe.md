# 归一化与位置编码

## 归一化
1. 归一化能够拉平不同维度的差距，不同特征产生的权重绝对大小可能有很大差异，如果不归一化最后的结果可能会被某几个特征的绝对大小所主导，掩盖了其他的特征。
其他的作用还有：
2. 稳定梯度传播，将子层输入拉到均值为 0，方差为 1 的标准正态分布（Standard Normal Distribution）附近，防止梯度爆炸或者消失。
3. 加速收敛，可以平衡所有的特征信息来进行预测，避免某个特征的权重过大而导致的预测结果偏差，学习率可快速稳定。
4. 正则化：减少模型对个别极端特征的依赖，降低过拟合的风险。
### norm 为什么NLP 用 LayerNorm 不用 BatchNorm？
LayerNorm 是对单个样本的所有特征维度一起求均值/方差进行归一化；BatchNorm 是对一个 batch 内同一维度求均值/方差。NLP 里序列长度不一且有 padding，会让 BN 统计量被 padding/混合语料干扰；而自回归推理常是 batch=1，BN 训练/推理统计方式不一致。
1. 统计量的意义不同
- cv：图片的batch统计量是有意义的（这张图的亮度平均值 vs 那张图的亮度平均值，代表了整体光照分布）。
- nlp：文本的一个 Batch 往往是硬凑的（一句话讲体育，一句话讲金融）。计算这堆杂乱句子的“平均词向量”没有任何语义价值，反而引入了噪声。
2. inference的问题
- 训练时用 Batch 统计量，推理时用 Running Mean/Var（滑动平均）。
- 对于 Transformer 这种对位置极度敏感的模型，如果推理时的统计量和训练时稍微对不上，生成的文本就会逻辑崩坏。LayerNorm 对每个样本独立计算，训练和推理完全一致


LN = ( x - mean ) / sqrt( var + eps ) * gamma + beta
eps是防止分母为0，gamma和beta是可学习的参数，为了使归一化能够偏向于更优的区间。
### Pre-Norm (GPT-2以后) 和 Post-Norm (BERT) 的区别？
归一化位置在子层之前称为pre-norm，在子层之后称为post-norm。
pre更主流，训练更稳定；post在精细 warmup/正则下可能达到更高性能，但训练不稳定。
- post：原始的transformer模型，先做子层计算并与残差相加，再做归一化，即 y = norm(x + sublayer(x))。如果能仔细调整warmup，效果往往更好
- pre：深层更新幅度变小（更接近恒等映射），因为每层都在加残差且先归一化，原始输入 $x$ 的成分占比变高，而深层网络的贡献被缩小了。这导致深层参数的梯度很小，有时候深层网络好像“没学到东西”。虽然 Pre-Norm 理论上限略低，但它不用调参就能跑通，极大地降低了训练模型的炸炉风险。
#### pre
当前比较主流的选择，y = x + sublayer(norm(x))
1. 稳定梯度：残差连接提供梯度的高速公路，梯度传播无额外的缩放，残差路径直接传递梯度，避免归一化对梯度的挤压，无需warm up
2. 训练稳定：每一层的输入都经过归一化，无论网络多深，输入区间始终稳定
有一种理解思路来解释为什么preNorm训练稳定：preNorm随着模型的层数增加可以近似为宽度增加而不是深度增加，展开单层的变换公式后可以看到深层网络的输出是初始输出与所有前置层变的累加和，每个残差项在归一化后量级较小，当网络层数很大的时候新加入的单个项对输出的相对改变量很小，对于深层网络相邻层输入变得几乎相同。如果输入几乎相同那两层近似等同与在一个归一化操作上并行的以后公用两个变换并将结果相加。
#### post
y = norm(x + sublayer(x))
1. 梯度缩放累积，每一层的梯度都要经过norm，深层网络中容易出现梯度逐渐衰减。
2. 训练不稳定，子层的输入未归一化，浅层网络中特征分布波动大，甚至几乎无法收敛。
### Llama 为什么用 RMSNorm？
1. 去中心化并不重要，layernorm中减去均值这一步对模型的贡献小。
2. RMSnorm相对于layernorm的计算量减少，使用均方根来代替了均值和方差的计算。
rms = x / (x.pow(2).mean().sqrt() + eps) * gamma
省略了beta是为了减少参数冗余，RMSNorm 是一种强制的缩放不变性（Scale Invariance），对于大模型缩放的小扰动比平移要重要得多。
## 位置编码
transformer在处理序列的时候感知不到位置信息，当你把 token 顺序打乱，注意力矩阵/输出会随同以相同方式被打乱，模型本身无法仅凭结构识别绝对位置/顺序，所以需要加入位置编码来引入位置信息。
### 绝对位置编码 (Sinusoidal)
最早在transformer中提出了绝对位置编码，通过在embedding之前给token序列加上一个位置矩阵来注入位置信息，但是缺点是外推性能差，训练时见过长度512，推理时遇到513就懵了，因为他没见过第513个位置的编码，所以在实际应用中一般会使用一个可学习的位置矩阵来进行位置编码，可学习的位置矩阵比固定的正弦编码更容易训练和收敛。
### 相对位置编码 (ALiBi)
绝对位置编码是把序列的绝对位置信息注入到token中来表示位置，而相对位置编码是通过token之间的相对位置关系来体现的，更重要的是位置编码的位置可以在生成QKV矩阵之后进行嵌入，避免了矩阵映射对位置信息的损坏。
相对位置编码的方法有很多，这里用ALiBi来介绍。
ALiBi是一种相对位置编码方法，它的核心思想是在softmax之前加上一个线性偏置惩罚。
核心逻辑：基于一个假设，距离越远的token相关性通常越低，所以他在Q@K的结果矩阵上直接减去一个与距离成正比的值。
scoreij = qi * kj + m * (-(i - j))
m是一个特定的斜率，对于不同的head有不同的m值
- 为什么Alibi能外推？
因为他不依赖绝对位置i或j的具体数值，只依赖i - j，即使推理时距离超过了巡礼那时的最大值，这种越远分越低趋势依然成立
### 旋转位置编码 (RoPE)
当前应用最多的编码方式，通过将向量旋转一定的方向来表示位置信息。
rope通过绝对位置编码的方式实现了相对位置编码的效果
- 实现方式（绝对）：给每个token的QK上乘了一个旋转矩阵Rm（Rm 只取决于它自己的位置m）
- 数学效果（相对）：当计算qm * kn时 （Rm * q）^ T * (Rn * k) = q ^ T Rn-m * k。最后的结果里面只剩下了n - m，也就是相对距离
可以先理解二维向量的旋转，再将多为向量两两配对组合就实现了旋转位置编码。
### 扩展（YaRN）
为了解决Rope外推性能不佳的问题，使用非线性插值来解决。
因为生成的旋转矩阵频率是固定的，一旦输入序列长度大于预定长度，就导致超出的部分位置信息和序列头的部分相同（三角函数的周期性）
YaRN根据不同维度的特征采用不用的缩放策略
对于低频分量使用简单的系数进行过缩放，对于高频分量使用复杂的缩放函数，因为低频分量的语义理解更重要，高频分量对于位置敏感。
低维度的快速旋转适合捕捉局部的位置关系，高维度的慢速旋转适合捕捉全局的位置关系。高频细节的重要性：相邻或相似的token需要依赖高频成分来区分它们的相对位置，低维的最小旋转角度不能过小（即频率不能过低），否则网络无法检测到细微的位置差异。
RoPE 里“插值”本质上不是在 token 序列上做插值，而是在做一件更直接的事：把某些维度的“相位增长速度”变慢，让它们在更长位置上仍处在训练时见过的相位范围里。

1) 先看 RoPE 在干什么
对第 i 个频率（对应某一对维度），RoPE 给位置 p 一个旋转角：θ_i(p) = p * ω_i
ω_i 越大 → 旋转越快（高频，通常对应低维那几对）
ω_i 越小 → 旋转越慢（低频，通常对应高维那几对）
注意力里用到的是相对角差：Δθ_i = (p - q) * ω_i
当上下文长度从训练的 L_train 拉到 L_test（更长）时，很多 Δθ_i 会落到训练从没见过的范围里，而且高频的相位会绕圈（模 2π）更快，出现“别名/混淆”（不同距离映射到相近角度）。
2) “插值”怎么做：缩放位置或缩放频率
最常见的做法等价于下面两种写法之一（效果一样）：
缩放位置：θ_i(p) = (p / s) * ω_i
或缩放频率：θ_i(p) = p * (ω_i / s)
这里 s > 1 可以理解为“把有效长度压回训练长度”。如果你想把长度从 L_train 扩到 L_test，常用 s ≈ L_test / L_train。
这会带来一个非常关键的效果：
Δθ_i = (p - q) * ω_i / s 变小了
等价于该维度的“波长”变大了（更不容易绕圈），从而更能表示长距离差异
3) 为什么 YaRN 要“保护高频、拉伸低频”
如果你对所有频率都用同一个 s 去缩放：
高频（局部语法/邻近关系用得多）也被“变慢”了 → 相邻 token 的角差变小，局部位置信号变钝，模型原来学会的局部模式会被破坏。
YaRN 的做法可以理解为：不要用一个统一的 s，而是用“按频率/维度变化的缩放系数”：
高频维度：s_i ≈ 1（几乎不插值，保持训练时的局部几何）
低频维度：s_i 接近目标缩放 s（强插值，把全局范围拉长）
中间频段：用一个平滑过渡
写成式子就是把角度改成：
θ_i(p) = (p / s_i) * ω_i，其中 s_i 随维度从 1 → s 平滑变化。
4) 直觉小例子（为什么能起效）
假设训练长度 4k，你要跑到 32k，s=8。
对低频维度用 s_i=8：Δθ 变成原来的 1/8，意味着这些维度在 32k 范围内“看起来”仍像在 4k 内变化，不容易绕圈、能承载长程差异。
对高频维度用 s_i≈1：相邻 token 的相位差几乎不变，局部顺序/语法仍是模型熟悉的那套。
所以 YaRN 不是“插值把高频搞复杂、低频搞简单”，而是决定哪些频段该被压缩（拉伸波长），以及在频段之间怎么平滑过渡，从而同时保住局部能力并扩展全局范围。


## 几个问题
### RoPE的模长去哪了
模长完全不变，RoPE使用的是旋转矩阵，正交矩阵的逆等于它的转置。
这对于训练是很重要的，计算注意力时会dk ** 0.5缩放，为了控制方差，Sinusoidal 编码是把位置向量 加（Add） 到词向量上，这会改变向量的模长和方差，可能干扰模型的数值稳定性。RoPE 只是旋转，完美的保留了信号的幅度信息，这对训练的稳定性极大（尤其是深层网络）。

### ALiBi 这么好，为什么 LLaMA 不用
因为ALiBi对长距离信息的惩罚太过了，导致模型看不见以前的信息
它强制规定距离越远分扣的越多。
RoPE 的优势：
RoPE 只是旋转。只要 $q$ 和 $k$ 转到了对应的角度，哪怕相隔 10 万米，它们的点积依然可以很大（甚至因为周期性重合）。
结论： ALiBi 适合局部关注的任务（如写代码补全），但对于需要全局理解的大模型（如阅读理解、长文本分析），RoPE 是更好的选择。

### 位置编码与显存的隐秘关系
如果不做优化，"广播机制" (Broadcasting) 会撑爆显存
假设 Batch=1, Head=32, Seq_Len=100k, Head_Dim=128。 RoPE 的 cos 和 sin 表原本很小：[100000, 128] (约 50MB)，这没问题。
在计算 RoPE 时，为了让 cos 表能和 $x$ [1, 32, 100000, 128] 相乘，通常会做 broadcast：
```
# 把 cos 表扩充到和 x 一样大
cos_expanded = cos[None, None, :, :].expand(B, H, L, D) 
x_rotated = x * cos_expanded + ...
```
显存爆炸点：
这个 cos_expanded 看起来只是个视图，但如果后续操作触发了**实具体化（Materialization）**或者自动求导保存了中间状态：
它的大小是：$32 \times 100,000 \times 128 \times 4 \text{ bytes} \approx 1.6 \text{ GB}$。
注意！这只是 1 层！
LLaMA-2-70B 有 80 层。
$1.6 \text{ GB} \times 80 = 128 \text{ GB}$。
仅仅为了存这个临时的位置编码矩阵，就需要两块 A100 显卡！ 这还没算 KV Cache 和模型权重。
解决方法： 必须使用 Compute-on-the-fly 或者 Triton/CUDA Kernel。 在底层 Kernel 里，我们不需要把 cos 表扩充成大矩阵，而是让每个线程去查那个小小的 [L, D] 表，读出来直接乘。 “用计算换显存”